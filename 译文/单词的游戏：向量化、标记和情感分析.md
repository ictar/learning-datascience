原文：[A Game of Words: Vectorization, Tagging, and Sentiment Analysis](https://towardsdatascience.com/a-game-of-words-vectorization-tagging-and-sentiment-analysis-c78ff9a07e42)

---

又名：利用自然语言处理，分析_权力的游戏_第一本书中的单词

![](https://cdn-images-1.medium.com/max/1200/1*RI5YClBztuwe2M7a0v1ZKw.jpeg)

↑ 图片来自 [Pixabay](https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4180794) 的 [simisi1](https://pixabay.com/users/simisi1-5920903/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4180794)

Full disclosure: 我并没有看过或者读过权力的游戏，但我希望通过分析它的文本来了解它。如果想要了解关于基本文本处理的更多背景知识的话，那么，可以读读我[其他的文章](https://medium.com/@madelinemccombe/text-processing-is-coming-c13a0e2ee15c)。权力的游戏五部书的完整文本可以在 [Kaggle](https://www.kaggle.com/khulasasndh/game-of-thrones-books#005ssb.txt) 上找到。在这篇文章中，我将使用已清理的文本，并使用它来解释以下概念：

  * 向量化：Bag-of-Words、TF-IDF 和 Skip-Thought 向量
  * 向量化之后
  * 词性标注（POS tagging）
  * 命名实体识别（NER）
  * Chunking and Chinking
  * 情感分析
  * 其他 NLP 包



* * *

### **向量化**

现在，我有一个词形词列表，但是，要怎样将其结构化，以使得机器能够理解它们？下面，我将探索一些**向量化方法**，这些方法将一个单词列表转换为一个数字数组，以便用于不同的机器学习方法。在此过程中，务必确保从文本中删除停用词和其他非必要单词，以便创建的数组/向量系统仅有要建模的重要维度。

一种方法是 **Bag-of-Words（词袋法）**，它定义一个由文本中包含的唯一字组成的字段，然后查找每个词语在文本中的计数。例如，假如我收集_权力的游戏_的唯一字列表，然后按章节将完整的列表分割成单词，最后我会得到一个数组，每一行是一个章节，而每一列是一个单次的计数。该方法的缺点是，它不保留单词的顺序（[fall, death] 和 [fall, love] 具有完全不同的含义），并且这个方法也不会捕获单词的任何实际含义。此外，如果按章分割文本，那么单词越多的章节将无意中具有更重的加权，因为在整个行中具有更高的计数。但是，这仍然是一种查看词语分布的好方法，并且当你希望查看特定单词出现的次数时尤为有用。下面是该方法在权力的游戏的文本上的一个实现，按章分割：

```py
    from sklearn.feature_extraction.text import CountVectorizer

    bow = CountVectorizer()  
    BOW = bow.fit_transform(page_lemm)  
    bagOFwords = pd.DataFrame(BOW.toarray())  
    bagOFwords.columns = bow.get_feature_names()
```

在上面的例子中，`page_lemm` 是一个长度为 572 （页数）的列表，其中，每个元素是该页上的单词串。`CountVectorizer()` 函数自动分割并统计所有单词串中的单词。在使用上面的代码之前，我先进行了停用词的移除和词形还原，相关代码可以在[我的 github](https://github.com/madelinemccombe/LaunchDS/blob/master/TextAnalysis_GoT2.ipynb) 上面找到。这份代码创建了一个 dataframe，其中，每一行对应书里的一个章节，每一列对应文本中的一个唯一字。该 dataframe 的主体包含每个章节中每个单词的计数。

解决 Bag-of-Words 的一些问题的另一个方法是 [**TF-IDF**（term frequency–inverse document frequency）](https://zh.wikipedia.org/wiki/Tf-idf)。 TF-IDF 与前一种方法类似，不同之处在于，每一行的每一列值按文档中单词的数量和单词的相对稀有度进行缩放。**词频（Term frequency）**等于文档中单词出现的次数除以文档中的单词总数。**逆向文件频率（Inverse document frequency）**计算语料库中所有文档的稀有单词的权重。稀有单词具有较高的 IDF 得分，而那些在语料库中的所有文档中都出现的单词的 IDF 得分近乎为零。这使得那些可能具有丰富含义的稀有单词在最终的分析中仍然具有影响。试着这样想，知道一本书中的所有章节都用了 “hand” 这个单词会更好吗？或者知道 “death” 只出现在其中 10 个章节会更有影响？将 TF 和 IDF 相乘以获得最终的 TF-IDF 得分。可以 [在这里找到](https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/)该方法的过程。Python 的 scikit-learn 包（`sklearn`）有一个函数 `TfidfVectorizer()`，它会帮你计算 TF-IDF 得分，如下所示：

```py
    from sklearn.feature_extraction.text import TfidfVectorizer

    vectorizer = TfidfVectorizer()  
    got_tfidf = vectorizer.fit_transform(page_lemm)  
    tfidf = pd.DataFrame(got_tfidf.toarray())  
    tfidf.columns = vectorizer.get_feature_names()
```

如你所见，这两个方法的代码非常像，都采用了相同的输入，但是在 dataframe 中提供了不同的内容。不是每个单词的计数，而是计算 TF-IDF 得分。下面是根据平均 Bag of Words 计数的前 10 个单词和根据平均 TF-IDF 得分计算的前 10 个单词的对比。有一些重叠，但是 TF-IDF 给出的名字字符的平均分高于 Bag of Words。我高亮了两种方法之间_不_重叠的单词。

![](https://cdn-images-1.medium.com/max/800/0*PEPUD-YpikN87Mkr)

**Skip-Thought Vectors** 是另一种向量化方法，它使用神经网络中的转移学习法来预测句子的周围。**转移学习（Transfer learning）**是指机器可以将它从一个任务中“学到”的东西应用到另一个任务上。这几乎是所有机器学习技术背后的思想，因为我们试图让以比人类更快更可量化的速度进行学习。特别对于文本处理，其思想是，算法构建出一个从数千本不同的书籍中学习到的神经网络，并计算出句子结构、主题和一般模式。然后，就可以将该算法应用于它尚未读过的书，以预测或者模拟文本中的情感或主题。你可以[阅读更多](https://monkeylearn.com/blog/beginners-guide-text-vectorization/)关于该方法的信息，以及它与 TF-IDF 的对比。

另一种严重依赖神经网络的矢量化方法是 **word2vec**，它计算两个单词之间的余弦相似度并在空间中绘制这些单词，以便将相似的单词组合在一起。你可以读一读[该方法](https://medium.com/@paritosh_30025/natural-language-processing-text-data-vectorization-af2520529cf7)的简洁实现。

* * *

### **向量化之后**

现在，你已经有了一组数字，那么接下来呢？通过 Bag of Words，你可以执行逻辑回归或者其他分类算法来看看数组中的哪些文档（行）最相似。这对于尝试查看两篇文章主题上是否相关的时候可能有用。Skip Thought Vectors 和 Word2Vec 都会基于文本中的含义来聚类单词，这是一种称为**单词嵌套**的方法。这种技术很重要，因为它保留了单词之间的关系。特别是在处理评论文本数据（任何带有数值评分的文本评论）的时候，这些技术可以产生关于消费者感受和想法的有价值的结果。由于_权力的游戏_并没有分类默认值，因此，我无法验证模型。下面，我会解释分析文本的替代方法。

* * *

### **词性标注（POS tagging）**

**词性标注（Part of Speech tagging 或者 POS tagging）** 是利用上下文线索，将词性分配给列表中的每个单词。这很有用，因为不同词性的同一个词可能具有两个完全不同的含义。例如，如果你有两个句子（“A plane can fly” 和 “There is a fly in the room”），那么，为了确定这两个句子是怎样关联的（或者是没关系），正确定义 “fly” 和 “fly” 就很重要了。通过词性标注单词允许你进行 chunking 和 chinking，这个我们稍后会解释。一个重要的注意事项是，词性标注应该在 _标记化之后并且在删除任何单词之前直接_ 进行，以便保持句子结构，并且单词属于哪种词性会更明显。进行这项操作的一种方法是使用 `nltk.pos_tag()`：

```py
    import nltk

    document = ' '.join(got1[8:10])  
    def preprocess(sent):  
      sent = nltk.word_tokenize(sent)  
      sent = **nltk.pos_tag**(sent)  
      return sent

    sent = preprocess(document)  
    print(document)  
    print(sent)
```

_[‘“Dead is dead,” he said. “We have no business with the dead.” ‘, ‘“Are they dead?” Royce asked softly. “What proof have we?” ‘]_

_[,…(‘``’, ‘``’), (‘We’, ‘PRP’), (‘have’, ‘VBP’), (‘no’, ‘DT’), (‘business’, ‘NN’), (‘with’, ‘IN’), (‘the’, ‘DT’), (‘dead’, ‘JJ’), (‘.’, ‘.’), (“‘’”, “‘’”),…]_

这是我们上面创建的结果片段，你可以看到形容词表示为 “JJ”，名词表示为 “NN”，以此类推。后面分块的时候将会使用这些信息。

* * *

### **命名实体识别**

有时，为特殊单词进一步定义词性是很有用的，特别是在尝试处理关于当前事件的文章时。除了是名词外，“London”、“Paris”、“Moscow” 和 “Sydney” 都是具有特殊含义的地名。人名、组织、事件、金钱、百分比和日期等也是如此。这个过程在文本分析中很重要，因为它可以成为理解文本块的一种方式。通常，要将 NER 应用到文本上，必须先前已执行标记化和词性标记。nltk 包内置了两种进行 NER 的方法，[这篇文章](https://medium.com/explore-artificial-intelligence/introduction-to-named-entity-recognition-eda8c97c2db1) 对很好的解释了它们。

执行 NER 并具有可视化和排序结果能力的另一种有用方法是通过 spaCy 包。[在这里可以找到](https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da) 一个很好的预排。我用这种方法探索了 GOT 文本，然后得到了一些有趣的结果：

```py
    import spacy  
    from collections import Counter  
    import en_core_web_sm  
    nlp = en_core_web_sm.load()  
    from pprint import pprint

    doc = nlp(document3)  
    pprint([(X.text, X.label_) for X in doc.ents])
```

_[(‘George R. R. Martin ‘, ‘PERSON’),  
(‘Ser Waymar Royce’, ‘PERSON’),  
(‘fifty’, ‘CARDINAL’),  
(‘Will’, ‘PERSON’),  
(‘Royce’, ‘PERSON’),  
(‘Eight days’, ‘DATE’),  
(‘nine’, ‘CARDINAL’),  
(‘Waymar Royce’, ‘PERSON’),  
_**_(‘Gared’, ‘PERSON’),  
(‘Gared’, ‘ORG’),_**_  
(‘forty years’, ‘DATE’),…]_

在上面的代码中，`document3` 是单个字符串，包含了 _权力的游戏 _ 的完整文本。这个包有效地查找并分类了所有实体类型。它对于 Gared 的某些情况下有点困惑（在某处，将他归类为 PERSON，另一处归类为 ORG，而另一个后面的地方归类为 WORK_OF_ART）。然而，总的来说，与仅仅使用词性标注相比，这种方法更深入了解文本内容。 每种实体类型的匹配数量和找到的数量最多的实体显示如下。不出所料，文本里发现了很多名字。

```py
    labels = [x.label_ for x in doc.ents]  
    items = [x.text for x in doc.ents]

    print(Counter(labels))  
    print(Counter(items).most_common(5))
```

_Counter({‘CARDINAL’: 340, ‘DATE’: 169, ‘FAC’: 34, ‘GPE’: 195, ‘LAW’: 2, ‘LOC’: 24, ‘MONEY’: 1, ‘NORP’: 32, ‘ORDINAL’: 88, ‘ORG’: 386,_**_ ‘PERSON’: 2307_**_, ‘PRODUCT’: 35, ‘QUANTITY’: 23, ‘TIME’: 86, ‘WORK_OF_ART’: 77})_

_[(‘_**_Jon_**_’, 259), (‘_**_Ned_**_’, 247), (‘_**_Arya_**_’, 145), (‘_**_Robert_**_’, 132), (‘_**_Catelyn_**_’, 128)]_

* * *

### **Chunking 和 Chinking**

chunking 和 chinking 是用于从文本中提取有意义的短语的两种方法。它们结合了词性标注和 Regex 来生成与请求的短语结构相匹配的文本片段。分块（Chunking）的一种实现是找到提供不同名词描述的短语，称为**名词短语分块**。一个名词短语块通常由一个限定成分或者所有格、形容词、一个可能的动词和名词组成。如果发现块中有不想要的部分，或者宁愿在特定的 POS 上分割文本，那么，实现目标的的一个简单方法是 **chinking**。它定义一个在分块的时候应该删除或者拆分的小块（称为一个 chink）。我不打算在本文里讨论 chinking，但是你可以[在这里](https://pythonprogramming.net/chinking-nltk-tutorial/)找到一份相关教程。

使用 NLTK 进行特定类型分块的最简单的方法是使用 `nltk.RegexpParser(r‘<><><>’)`。它允许指定名词短语公式，并且很容易解析。每个 <> 指向要匹配的单词的词性，并且在每个 <> 上应用正常的正则表达式语法。这与 `nltk.Text().findall(r’<><><>’)` 的概念非常类似，但是这里使用的是 POS，而不是实际的单词。在创建要解析的正则表达式字符串时，要注意几点：词性缩写（NN = 名词，JJ = 形容词，PRP = 介词等）可能因包而异；并且有时候开始使用更具体的正则表达式，然后再扩大搜索范围会更好些。如果你现在一头雾水，那么，可以[这里](https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b)有对于这个概念不错的介绍。此外，最好先重温一下句子结构和词性，以便完全理解分块返回的内容。下面是将其应用到 GOT 上的一个例子：

```py
    document2 = ' '.join(got1[100:300])  
    big_sent = preprocess(document2) # POS tagging words

    pattern = 'NP: {<DT>?<JJ>*<NN.?>+}'  
    cp = nltk.RegexpParser(pattern)  
    cs = cp.parse(big_sent)  
    print(cs)
```

_(…, (_**_NP Twilight/NNP_**_) deepened/VBD ./. (_**_NP The/DT cloudless/NN sky/NN_**_) turned/VBD (_**_NP a/DT deep/JJ purple/NN_**_) ,/, (_**_NP the/DT color/NN_**_) of/IN (_**_NP an/DT old/JJ bruise/NN_**_) ,/,…)_

这与 NER 非常像，因为也可以将 NN 或者 NNP（名词或者专有名词）组合在一起，以查找对象的全名。此外，匹配的模式可以是词性的任意组合。这在查找某些类型的短语时很有用。然而，如果词性标记不正确，那么，你将无法找到要查找的短语类型。这里，我只查找名词短语，但我的 [github 代码](https://github.com/madelinemccombe/LaunchDS/blob/master/TextAnalysis_GoT2.ipynb) 中包含更多类型的块。

* * *

### **情感分析**

**情感分析** 是计算机如何将目前为止涵盖的一切组合起来，然后提出一种方法，来传达一个段落的整体要点。它将句子、段落或者是其他文本集中的单词与字典中的单词列表进行比较，然后基于句子中单个单词的分类方式来计算情感得分。这种方法主要用于分析评论、文章和其他观点，但今天，我要将它用在 GOT 上。我主要想知道，这本书的整体基调是正面的还是负面的，并且这种基调是否不同章节不一样。有两种进行情感分析的方法：你可以在先前分类好的文本上训练和测试模型，然后使用该模型来预测相同类型的新文本是正面的还是负面的；或者，也可以简单使用函数中内置的现有词典来分析和报告正面分数和负面分数。下面这个例子分析了 _权力的游戏_ 第一页中一些句子：

```py
    from nltk.sentiment.vader import SentimentIntensityAnalyzer  
    nltk.download('vader_lexicon')

    sid = **SentimentIntensityAnalyzer**()  
    for sentence in sentences:  
      print(sentence)  
      ss = sid.polarity_scores(sentence)  
      for k in sorted(ss):  
        print('{0}: {1}, '.format(k, ss[k]), end='')  
      print()
```

_…“Do the dead frighten you?”   
compound: _**_-0.7717_**_, neg: 0.691, neu: 0.309, pos: 0.0,   
Ser Waymar Royce asked with just the hint of a smile.   
compound: _**_0.3612_**_, neg: 0.0, neu: 0.783, pos: 0.217,   
Gared did not rise to the bait.   
compound: _**_0.0_**_, neg: 0.0, neu: 1.0, pos: 0.0,…_

由于这是分析书籍文本，而不是评论文本，故而很多句子都会具有一个中性复合分数（0）。这完全没问题，因为我只是在寻找书中语言的一般趋势。但看到提及死亡时，应用了负值，这仍然令人觉得高兴。

**TextBlob** 是另一个可以执行情感分析的有用的包。一旦将文本转换成 TextBlob 对象（`textblob.textBlob()`），就可以标记化、词形还原、标记纯文本和制作 WordNet（量化单词间相似性）。这个包有许多不同的特有文本对象，允许我们进行一些非常酷的转换，[相关解释可以看这里](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis)。甚至还有一个 `correct()` 函数，尝试纠正拼写错误。我不打算在这篇文章中讨论太多关于它们的信息，因为我在尝试分析的是一本通常应该有蒸汽拼写和语法的书，但是，在处理特别凌乱的文本数据时，这些工具中有许多有用的。下面是在 _权力的游戏_ 的第一页上进行情感分析的 TextBlob 版本：

```py
    from textblob import TextBlob  
    def detect_polarity(text):  
        return TextBlob(text).sentiment  
    for sentence in sentences:  
      print(sentence)  
      print(detect_polarity(sentence))
```

_“Do the dead frighten you?”   
Sentiment(polarity=_**_-0.2_**_, subjectivity=0.4)   
Ser Waymar Royce asked with just the hint of a smile.   
Sentiment(polarity=_**_0.3_**_, subjectivity=0.1)   
Gared did not rise to the bait.   
Sentiment(polarity=_**_0.0_**_, subjectivity=0.0)_

nltk 和 textblob 的情感分数之间具有相似性，但是因为 nltk 版本是一个复合分数，故而具有更多的可变性。而 textblob 情感有一个主观分数，这有助于说明分类句子的准确度。下面，是每个方法按页情感分布。Textblob 总体上给出了更高的情感评级，而 nltk 得分差异更大些。

![](https://cdn-images-1.medium.com/max/800/1*FBdbcpXYnxaG0KEh5026HA.png)

如果你试图从社交媒体或者表情符号中收集情绪信息，那么 [**VADER 情感分析**](https://github.com/cjhutto/vaderSentiment) 是一个专为此任务而生的工具。它内置了俚语（lol、omg、nah、meh 等），甚至可以理解表情符号。可以[在这里找到](https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f)它的使用方法。此外，如果 Python 并非你用来进行文本分析的语言，那么，在不同的语言或者软件中海油其他方法可以进行情感分析，[这里对其进行了解释](https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-2-7f3a75c262a3)。

* * *

### **其他 NLP 包**

在这篇文章中，我只解释了 `nltk`、`textblob`、`vaderSentiment`、`spacy` 和 `sklearn` 包中的函数，但是取决于你要完成的任务，它们各自有利有弊。其他可能更适合你的任务的包是 Polyglot 和 Genism。[**Polyglot**](https://pypi.org/project/polyglot/) 因分析大量语言（根据任务支持 16 到 196 种）的能力而出名。[**Genism**](https://pypi.org/project/gensim/) 主要用于文本上无监督学习的任务。你可以在[这里](https://medium.com/activewizards-machine-learning-company/comparison-of-top-6-python-nlp-libraries-c4ce160237eb)找到包含所有这些信息的便捷图表。

* * *

### **总结**

在写这篇文章时，我学到的关键点是，完成一个任务总是有至少三种方法，而最佳选项仅仅取决于所使用的数据类型。有时，你会优先考虑计算时间，而有时，你需要一个可以很好地进行无监督学习的包。文本处理是一门迷人的科学。我迫不及待地想看看它在未来几年里的发展方向。在这篇文章中，我介绍了向量化以及如何确定文本之间的相似性，标记以允许为单词附加意义，还有情感分析，它大致描述了文本正负面。我从“权力的游戏”收集了许多见解，例如，其中有许许多多的死亡、sir 是一个拼写为 Ser 的总称、以及其中并没有像我所认为的那样多的龙。但是，这可能说服我现在就去读它的系列书！希望你能喜欢这篇文章！

可以在 [github 这里](https://github.com/madelinemccombe/LaunchDS/blob/master/TextAnalysis_GoT2.ipynb) 找到我的代码副本，里面包含了更多的例子和解释！请随意使用。