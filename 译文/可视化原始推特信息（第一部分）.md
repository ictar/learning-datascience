原文：[Visualisation of Information from Raw Twitter Data — Part 1](https://towardsdatascience.com/visualization-of-information-from-raw-twitter-data-part-1-99181ad19c)

---

又名：让我们探讨一下，可以从原始的推特数据中轻松检索哪些信息？


![](https://cdn-images-1.medium.com/max/1600/1*MV5TX9fv5_jjskoxGweDqg.png)

大家好！在之前的文章中，我们探讨了如何使用 Python 和下面这两个推特 API 来高效下载数据：[Streaming API ](https://medium.com/@jaimezornoza/downloading-data-from-twitter-using-the-streaming-api-3ac6766ba96c)用于下载实时生成的推文，而这个 [REST API](https://towardsdatascience.com/downloading-data-from-twitter-using-the-rest-api-24becf413875) 用于下载用户时间轴和关注者等历史信息。

本文是两篇系列文章中的第一篇。该系列致力于从使用推特的 Streaming API 下载的数据中，提取酷炫的可视化和信息，并且不包含任何机器学习或者任何其他复杂算法。

那么，让我们轻松简单地，开始吧！

#### 1. 收集数据：

首先，需要收集用于可视化信息的数据。我们将使用 Streaming API 来收集特定主题的推文，这可以使用 stream 对象的 filter 参数来查询特定的话题标签。

如果你对此不熟悉的话，建议你看下我这篇关于如何使用 Streaming API 来下载数据的文章：

[**使用 Streaming API 下载推特数据**  
_在这篇文章中，我们将介绍如何使用 Streaming API 来获取包含特定单词或主题标签的推文，以及如何……](https://medium.com/@jaimezornoza/downloading-data-from-twitter-using-the-streaming-api-3ac6766ba96c "https://medium.com/@jaimezornoza/downloading-data-from-twitter-using-the-streaming-api-3ac6766ba96c" )[](https://medium.com/@jaimezornoza/downloading-data-from-twitter-using-the-streaming-api-3ac6766ba96c)

基本上，推文下载脚本一般看起来像这样：

```py
    l = StdOutListener()
    auth = OAuthHandler(consumer_key, consumer_secret)    
    auth.set_access_token(access_token, access_token_secret)    stream = Stream(auth, l)
    stream.filter(track=tracklist)
```

包括以下代码：

```py
    tracklist = ['#HASHTAG1', '#HASHTAG2', '#HASHTAG3']
```

并且将 _#HASHTAG1, #HASHTAG2 和 #HASHTAG3_ 的值修改为想要收集的主题相关的主题标签。包含该列表或多或少个主题标签。

一旦脚本准备就绪，就可以像这样运行并输出到 .txt 文件中，正如上述文章所述：

**Twitter_Downloader.py > twitter_data.txt**

让它跑一段时间，或者设置最大推文数。就是这样，就能准备好要分析的数据了！

#### 2. 准备数据：

现在，数据已经就绪，让我们来看看可以从中获得什么见解！在这篇文章中，我下载了为期约两天的有关英国脱欧的推文。查询推文时使用与 **Brexit** 相关的主题标签（例如_ #Brexit、#brexit、#PeoplesVote 或者 #MarchToLeave_）。

打开 Jupyter Notebook，然后开始敲码：

首先，一如既往，我们需要导入分析和可视化所需的库：

```py
    #导入所有需要的库
    import pandas as pd  
    import numpy as np  
    import matplotlib.pyplot as plt  
    import json  
    import seaborn as sns  
    import re  
    import collections  
    from wordcloud import WordCloud
```

然后，读取所收集的数据，然后进行处理。记住，Streaming API 的输出是一个表示一条推文 JSON 对象，其中许多字段可以提供非常有用的信息。在下面的代码块中，我们从储存的 .txt 文件中读取这些推文：

```py
    #Reading the raw data collected from the Twitter Streaming API using #Tweepy.   
    tweets_data = []  
    tweets_data_path = 'Brexit_推文_1.txt'  
    tweets_file = open(tweets_data_path, "r")  
    for line in tweets_file:  
        try:  
            tweet = json.loads(line)  
            tweets_data.append(tweet)  
        except:  
            continue
```

对于上面的代码，你必须将字符串 **_推文_data_path_** 的值修改为你存储数据的那个文档的名字。我还建议你在存储所下载的推文数据的同一个目录下创建这个 notebook。这样，在加载数据的时候，就不必考虑任何相对路径。

下载数据时，可能会有连接问题或者其他类型的错误，因此，在我们存储推文的 .txt 中，可能会有些地方本应该是 _推文_，但却成了错误码。要清除它们，只需要在读取 .txt 文件之后运行以下代码。

```py
    #Error codes from the Twitter API can be inside the .txt document, 
    #take them off  
    tweets_data = [x for x in tweets_data if not isinstance(x, int)]
```

现在，看看我们收集了多少条 _推文_:
```py
    print("The total number of Tweets is:",len(tweets_data))
```

就我而言，我下载了 **133030 ** 条 _推文_。

好的，现在我们已经在 notebook 中读取了 .txt 文件，并且将推文存储为 JSON 格式。接下来，我们将实现一些函数，将这些 JSON 对象的一些参数映射到 Pandas dataframe 的列，该 datafrom 用以保存 _推文_ 信息。

首先，创建一个函数，允许我们查看所选的 _推文_ 是否是 _转发_。通过判断 JSON 对象是否包含一个 ‘_retweeted status’_ 字段来完成。如果该 _推文_ 是转发，那么函数返回一个布尔值 True，如果不是，则返回 False。

```py
    #创建一个函数来检查推文是否为转发
    def is_RT(tweet):  
        if 'retweeted_status' not in tweet:  
            return False        
        else:  
            return True
```

接下来，与此类似，评估下载的 _推文_ 是否是对其他用户 _推文_ 的回复。同样，这是通过检查 JSON 对象中是否缺少某些字段来完成的。我们将使用“_in_reply_to_screen_name_” 字段，这样，除了查看推文是否为回复之外，还可以看看该 _推文_ 是回复哪个用户。

```py
    #Create a function to see if the tweet is a reply to a tweet of 
    #another user, if so return said user. 

    def is_Reply_to(tweet):  
        if 'in_reply_to_screen_name' not in tweet:  
            return False        
        else:  
            return tweet['in_reply_to_screen_name']
```

最后，推文的 JSON 对象包含一个“_source_”字段，表示发布 _推文_ 的设备或者应用的标识符。推特没有提供关于如何将该 source 字段映射到实际设备的系统指导，因此根据我所认为的这些源信息应该映射到哪些设备，我手动整了一个列表。

```py
    #Create function for taking the most used Tweet sources off the 
    #source column 

    def reckondevice(tweet):  
        if 'iPhone' in tweet['source'] or ('iOS' in tweet['source']):  
            return 'iPhone'  
        elif 'Android' in tweet['source']:  
            return 'Android'  
        elif 'Mobile' in tweet['source'] or ('App' in tweet['source']):  
            return 'Mobile device'  
        elif 'Mac' in tweet['source']:  
            return 'Mac'  
        elif 'Windows' in tweet['source']:  
            return 'Windows'  
        elif 'Bot' in tweet['source']:  
            return 'Bot'  
        elif 'Web' in tweet['source']:  
            return 'Web'  
        elif 'Instagram' in tweet['source']:  
            return 'Instagram'  
        elif 'Blackberry' in tweet['source']:  
            return 'Blackberry'  
        elif 'iPad' in tweet['source']:  
            return 'iPad'  
        elif 'Foursquare' in tweet['source']:  
            return 'Foursquare'  
        else:  
            return '-'
```

好啦！在创建所有这些函数后，我们就准备好将推文传到 dataframe 中，以便处理。请注意，该 dataframe 的某些列是直接从 JSON 对象中提取的，并不需要自定义函数。因此，只需从 JSON 中获取这些列的原始数据即可。然而，该 dataframe 的某些列确实需要进一步的解析。

dataframe 的 “_text_” 列的值，可以使用 JSON 对象中普通的“_text_”字段，也可以使用“_extended_tweet”_ 全文。这样做是因为有些推文的长度在 140 到 280 个字符之间，对于它们来说，JSON 对象的“_text_” 字段不包含完整的文本。

```py
    #Convert the Tweet JSON data to a pandas Dataframe, and take the 
    #desired fields from the JSON. More could be added if needed.

    tweets = pd.DataFrame()  
    tweets['text'] = list(map(lambda tweet: tweet['text'] if 'extended_tweet' not in tweet else tweet['extended_tweet']['full_text'], tweets_data))  
    tweets['Username'] = list(map(lambda tweet: tweet['user']['screen_name'], tweets_data))  
    tweets['Timestamp'] = list(map(lambda tweet: tweet['created_at'], tweets_data))  
    tweets['lenght'] = list(map(lambda tweet: len(tweet['text']) if'extended_tweet' not in tweet else len(tweet['extended_tweet']['full_text']) , tweets_data))  
    tweets['location'] = list(map(lambda tweet: tweet['user']['location'], tweets_data))  
    tweets['device'] = list(map(reckondevice, tweets_data))  
    tweets['RT'] = list(map(is_RT, tweets_data))  
    tweets['Reply'] = list(map(is_Reply_to, tweets_data))
```

dataframe 的 _“Username_” 一列表示发推的用户，而所有其他列几乎都是不言自明的。

如果我们看看 dataframe 的头几行，它看起来应该像这样：

```py
    tweets.head()
```

![](https://cdn-images-1.medium.com/max/1600/1*igq34asgAo7Xvl0iSJzQFA.png)

棒棒哒！我们的 dataframe 已经构建完毕。至此，我们已经准备好探索数据啦！

#### 3. 数据分析和可视化

我们首先探索使用 Streaming API 可以检索的不同推文类别。例如，看看有多少 _推文_ 是 _转发_ 的：

```py
    #See the percentage of tweets from the initial set that are #retweets:

    RT_tweets = tweets[tweets['RT'] == True]  
    print(f"The percentage of retweets is {round(len(RT_tweets)/len(tweets)*100)}% of all the tweets")
```

从我的数据上看，**转发的推文占总推文的 73%**。

这提供了一些关于推特如何运行的有趣信息：大部分用户并不发布他们自己的内容，而是转发其他用户的内容。

如果我们获取 RT_tweets dataframe，然后打印它前几行数据，则会得到这样的结果：

```py
    RT_tweets.head()
```

![](https://cdn-images-1.medium.com/max/1600/1*A_qz_JsOYbUs4gUaRxgimw.png)

从这个 dataframe 中，我们可以看到，当 Streaming API 返回的 _推文_ 是 _转发_ 时，其文本结构是怎样的。格式如下：

> “RT @InitialTweetingUser: Tweet Text”

其中，开头的 _RT_ 表明，这样的 _推文_ 是一条 _转发_，_@InitialTweetingUser_ 是发布原始推文的账户的推特用户名，而 Tweet Text 是所述初始推文的文本。

通过我们创建的 Reply 列，还可以看到从 Streaming API 下载的推文中有多少是回复另一个用户的 _推文_：

```py
    #See the percentage of tweets from the initial set that are replies 
    #to tweets of another user:

    Reply_tweets = tweets[tweets['Reply'].apply(type) == str]  
    print(f"The percentage of retweets is {round(len(Reply_tweets)/len(tweets)*100)}% of all the tweets")
```

从我的数据上看，**回复大约占所有推文的 7%。**。

同样，如果我们看看这个 _Reply_推文_ dataframe，那么可以看到 Streaming API 返回的回复文本结构：

![](https://cdn-images-1.medium.com/max/1600/1*H0A2eVVuz5ojBBNIaN8V3A.png)

可以看到，这些回复有以下格式：

> “@InitialTweetingUser Reply Text”

其中，_@InitialTweetingUser_ 是被回复推文的作者，而 _Reply Text_ 是回复内容。

现在，让我们看看提到但不是转发的推文的百分比。请注意，这些 _推文_ 包括之前的回复推文。

```py
    #See the percentage of tweets from the initial set that have 
    #mentions and are not retweets:

    mention_tweets = tweets[~tweets['text'].str.contains("RT")  & tweets['text'].str.contains("@")]  
    print(f"The percentage of retweets is {round(len(mention_tweets)/len(tweets)*100)}% of all the tweets")
```

从我的数据上看，这占 **推文总数的 11%。**提到某人但既不是回复也不是转发的推文只是那些在文本中间某处包含所提及的人的推文，如：

> ‘Our working assumption remains that the UK is leaving on the 29th of March’, says @EU_Commission spox on #brexit. Nice use of the verb ‘remain’.

最后，让我们看看有多少 _推文_ 只是纯文本 _推文_，不提及任何人也不是转发：

```py
    #See how many tweets inside are plain text tweets (No RT or mention)

    plain_text_tweets = tweets[~tweets['text'].str.contains("@")  & ~tweets['text'].str.contains("RT")]  
    print(f"The percentage of retweets is {round(len(plain_text_tweets)/len(tweets)*100)}% of all the tweets")
```

从我的数据上看，这**占所有 _推文_ 总数的 15% 左右**。

让我们基于所有这些类别绘制一张图，以便更好地比较其比例：

```py
    #Now we will plot all the different categories. Note that the reply 
    #tweets are inside the mention tweets

    len_list = [ len(tweets), len(RT_tweets),len(mention_tweets), len(Reply_tweets), len(plain_text_tweets)]  
    item_list = ['All Tweets','Retweets', 'Mentions', 'Replies', 'Plain text tweets']  
    plt.figure(figsize=(15,8))  
    sns.set(style="darkgrid")  
    plt.title('Tweet categories', fontsize = 20)  
    plt.xlabel('Type of tweet')  
    plt.ylabel('Number of tweets')  
    sns.barplot(x = item_list, y = len_list,  edgecolor = 'black', linewidth=1)  
      
    plt.show()
```

![](https://cdn-images-1.medium.com/max/1600/1*Kjm-uJPP7aK3rUOXib1OeQ.jpeg)

棒棒哒！让我们发掘下哪些是最常用的主题标签和最常提及的用户：
```py
    #To see the most used hashtags.

    hashtags = []  
    hashtag_pattern = re.compile(r"#[a-zA-Z]+")  
    hashtag_matches = list(tweets['text'].apply(hashtag_pattern.findall))

    hashtag_dict = {}  
    for match in hashtag_matches:  
        for singlematch in match:  
            if singlematch not in hashtag_dict.keys():  
                hashtag_dict[singlematch] = 1  
            else:  
                hashtag_dict[singlematch] = hashtag_dict[singlematch]+1
```

为此，我们将使用正则表达式（包含在 **Python 库 re** 中），创建用于检测文本内主题标签的模式。然后，创建一个包含所有找到的主题标签的字典，其中，键是主题标签的文本，而值是该标签发布的次数。
```py
    #Making a list of the most used hashtags and their values

    hashtag_ordered_list =sorted(hashtag_dict.items(), key=lambda x:x[1])  
    hashtag_ordered_list = hashtag_ordered_list[::-1]

    #Separating the hashtags and their values into two different lists  
    hashtag_ordered_values = []  
    hashtag_ordered_keys = []  
    #Pick the 20 most used hashtags to plot  
    for item in hashtag_ordered_list[0:20]:  
        hashtag_ordered_keys.append(item[0])  
        hashtag_ordered_values.append(item[1])
```

此后，按值对字典进行排序，然后将值和主题标签分成两个不同的列表。通过这样，现在我们可以绘制前 20 个最常用的主题标签，以及它们出现的次数：

```py
    #Plotting a graph with the most used hashtags

    fig, ax = plt.subplots(figsize = (12,12))  
    y_pos = np.arange(len(hashtag_ordered_keys))  
    ax.barh(y_pos ,list(hashtag_ordered_values)[::-1], align='center', color = 'green', edgecolor = 'black', linewidth=1)  
    ax.set_yticks(y_pos)  
    ax.set_yticklabels(list(hashtag_ordered_keys)[::-1])  
    ax.set_xlabel("Nº of appereances")  
    ax.set_title("Most used #hashtags", fontsize = 20)  
    plt.tight_layout(pad=3)  
    plt.show()
```

![](https://cdn-images-1.medium.com/max/1600/1*LxwyPZQc1zQWLgFGyTmtXA.jpeg)

从这张图中可以清楚看到，**#Brexit** 是最常用的最提标签。这很明显，因为它是用于下载英国脱欧（Brexit）主题推文的标签之一。另一个可以将此信息展示为漂亮的可视化的是 Wordcloud（使用 Wordcloud Python 库）。 Wordclouds 是具有相应数值（如出现次数）的词云，并且单词大小按照对应数值进行缩放：具有最高值的单词将是词云中最大的词。

```py
    #Make a wordcloud plot of the most used hashtags, for this we need a #dictionary   
    #where the keys are the words and the values are the number of #appearances

    hashtag_ordered_dict = {}  
    for item in hashtag_ordered_list[0:20]:  
        hashtag_ordered_dict[item[0]] = item[1]  
    wordcloud = WordCloud(width=1000, height=1000, random_state=21, max_font_size=200, background_color = 'white').generate_from_frequencies(hashtag_ordered_dict)  
    plt.figure(figsize=(15, 10))  
    plt.imshow(wordcloud, interpolation="bilinear")  
    plt.axis('off')  
      
    plt.show()
```

![](https://cdn-images-1.medium.com/max/1600/1*maRDsdw4qJzWmGdcJ4pHJw.jpeg)

↑ 最常用的主题标签的 Wordcloud 展示

看起来很酷吧？现在让我们对“提到”进行同样的操作。

```py
    #Now we will do the same with the mentions:  
      

    mentions = []  
    mention_pattern = re.compile(r"@[a-zA-Z_]+")  
    mention_matches = list(tweets['text'].apply(mention_pattern.findall))

    mentions_dict = {}  
    for match in mention_matches:  
        for singlematch in match:  
            if singlematch not in mentions_dict.keys():  
                mentions_dict[singlematch] = 1  
            else:  
                mentions_dict[singlematch] = mentions_dict[singlematch]+1
```

同样，我们使用正则表达式来构建“提及”模式，并且创建一个字典，其中键是提到的用户，值是他们被提及的次数。考虑到这种“提及”模式也会从转发和回复中获取“提及”，因此，该字典还将包含那些未明确提及的用户，以及推文被转发或者回复的用户。
```py
    #Create an ordered list of tuples with the most mentioned users and #the number of times they have been mentioned

    mentions_ordered_list =sorted(mentions_dict.items(), key=lambda x:x[1])  
    mentions_ordered_list = mentions_ordered_list[::-1]

    #Pick the 20 top mentioned users to plot and separate the previous #list into two list: one with the users and one with the values

    mentions_ordered_values = []  
    mentions_ordered_keys = []  
    for item in mentions_ordered_list[0:20]:  
        mentions_ordered_keys.append(item[0])  
        mentions_ordered_values.append(item[1])
```

现在，如果绘制这些结果：
```py
    fig, ax = plt.subplots(figsize = (12,12))  
    y_pos = np.arange(len(mentions_ordered_values))  
    ax.barh(y_pos ,list(mentions_ordered_values)[::-1], align='center', color = 'yellow', edgecolor = 'black', linewidth=1)  
    ax.set_yticks(y_pos )  
    ax.set_yticklabels(list(mentions_ordered_keys)[::-1])  
    ax.set_xlabel("Nº of mentions")  
    ax.set_title("Most mentioned accounts", fontsize = 20)  
      
    plt.show()
```

![](https://cdn-images-1.medium.com/max/1600/1*VLAoqrpqlMaiEcLHNwNnfA.jpeg)

从中我们可以看到谁是最受关注的用户：@theresa_may（Theresa May 的官方账号）。还可以在这张图表中看到其他政治人物，像 @jeremycorbyn（Jeremy Corbyn）或者 @Anna_Soubry（Anna Soubry）、政党账号（@UKLabour、@Conservatives）、新闻来源（@BBCNews、@BBCPolitics、@SkyNews）和不同的记者（@georgegalloway）。

这种了解可能非常有用。在未来的帖子中，我们将探讨如何使用不同的推特互动来创建网络，并看看这些网络中这些最常被提及的用户的角色。

还可以为最常被提及的用户创建一个 WordCloud 展示：

```py
    #Make a wordcloud representation for the most mentioned accounts too

    mentions_ordered_dict = {}  
    for item in mentions_ordered_list[0:20]:  
        mentions_ordered_dict[item[0]] = item[1]  
    wordcloud = WordCloud(width=1000, height=1000, random_state=21, max_font_size=200, background_color = 'white').generate_from_frequencies(mentions_ordered_dict)  
    plt.figure(figsize=(15, 10))  
    plt.imshow(wordcloud, interpolation="bilinear")  
    plt.axis('off')  
      
    plt.show()
```

![](https://cdn-images-1.medium.com/max/1600/1*PpqebzYjDliHqxhJswm08g.jpeg)

↑ 最常被提及的用户的 Worcloud 展示

### 总结

我们已经探讨了一些有趣的可视化。这些可视化可以从原始的推特数据中获得，并且没有任何复杂的算法。还研究了推特的 Streaming API 的响应格式。

在下一篇文章中，我们将继续使用其他一些很酷的可视化技术：看看哪些用户发推最多、这些用户是机器人的可能性。我们还会创建一个推特发布的时间序列，检查发这些 _推文_ 所使用的设备都是哪些，并进一步的了解一些信息。

请随时在推特上关注我：@jaimezorno，或者通过 [Linkedin](https://www.linkedin.com/in/jaime-zornoza-0b1446116/) 与我联系。

感谢阅读，愿你有个美好的一天，再见！
